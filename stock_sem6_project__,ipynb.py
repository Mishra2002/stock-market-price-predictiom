# -*- coding: utf-8 -*-
"""STOCK_SEM6_Project__,ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-Aem_tSGWq6-xMJ_bx1LwgxjSU8D07qh

# Stock Market Prediction Using Machine Learning

<h3> Note :  </h3>   
Data Set link : 

<h3> Note :  </h3>   

- There are multiple variables in the dataset  as follows : 
  date, open, high, low, last, close, total_trade_quantity, and turnover.

- The columns Open and Close represent the starting and final price at which the stock is traded on a particular day.
- High, Low and Last represent the maximum, minimum, and last price of the share for the day.
- Total Trade Quantity is the number of shares bought or sold in the day 
-  Turnover (Lacs) is the turnover of the particular company on a given date.
- Another important thing to note is that the market is closed on weekends and public holidays
"""

# PLZ Go to data set  notebook

key = '9a47147e230b89a2ad261c9f3c5dc8e153a1d861'
key = '9a47147e230b89a2ad261c9f3c5dc8e153a1d861'

import pandas as pd
# for mathematical computation
import numpy as np
import math 

# for ploting and visualize data
import seaborn as sns 
from matplotlib import pyplot as plt

# for normalizing data
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler(feature_range=(0, 1))

# for get the stock data
import pandas_datareader as pdr

# we get from tiingo  : link -  https://www.tiingo.com/


plt.style.use('ggplot')

df = pdr.get_data_tiingo('INFY', api_key=key)

# converting into a csv file 
df.to_csv('INFY.csv')

# loading it in df variable
df = pd.read_csv('INFY.csv')

# printing the head
df.head(1)

df.tail(1)

"""### Insight : 
- We are having Stock details of "Infosys" for around  5 years , that is from 21/04/2017 to 19/04/2022
- we are taking "Open" as our target variable

### Exploratory data analysis:
- Getting to know the data
- Data preprocessing (Missing values)
- Cross tables and data visualization
"""

def eda(data):
    print("Size  : ",data.size)
    print("Shape : ",data.shape)
    print('-'*50)
    print("\nData types of the features: ")
    print(data.dtypes)
    print(df.info(verbose=False))
eda(df)

# checking if any null values present
print(df.isnull().sum())
sns.heatmap(df.isnull(),cbar=False)

for col in df.columns:
    print('{} : {}'.format(col,df[col].unique()))
# To check any abnormal values are there or not like "?"

print('\n Shape of the data:')
df.shape

df.columns

#find coorelation b/w all attributes
import seaborn as sns
import matplotlib.pyplot as plt
plt.figure(1 , figsize = (17 , 8))
cor = sns.heatmap(df.corr(), annot = True)

# Lets Plot Box Plot for wipro  :  outlier 
sns.boxplot(df.open)

#setting index as date
df['date'] = pd.to_datetime(df.date,format='%Y-%m-%d')
df.index = df['date']

#plot
plt.figure(figsize=(16,8))
plt.plot(df['open'], label='open Price history')

"""# MOVING AVERAGE """

#creating dataframe with date and the target variable
data = df.sort_index(ascending=True, axis=0)
new_data = df[['open']]
new_data = new_data.reset_index()
new_data[new_data['date']=='2021-04-21']

"""NOTE: While splitting the data into train and validation set, we cannot use random splitting since that will destroy the time component. So here we have set the last year’s data into validation and the 4 years’ data before that into train set.

"""

for i in range(0,len(data)):
     new_data['date'][i] = data['date'][i]
     new_data['open'][i] = data['open'][i]


# splitting into train and validation
train = new_data[:1007]
valid = new_data[1007:]

# shape of actual data
print('\n Shape of Data set:')
print(df.shape)
# shapes of training set
print('\n Shape of training set:')
print(train.shape)

# shapes of validation set
print('\n Shape of validation set:')
print(valid.shape)

"""- In the next step, we will create predictions for the validation set and check the RMSE using the actual values."""

#  making predictions
preds = []
for i in range(0,valid.shape[0]):
    a = train['open'][len(train)-251+i:].sum() + sum(preds)
    b = a/251
    preds.append(b)

# checking the results (RMSE value)
rms=np.sqrt(np.mean(np.power((np.array(valid['open'])-preds),2)))
print('\n RMSE value on validation set:')
print(rms)

df['open'].plot(xlim=['2021-04-22','2022-04-19'])

valid['Predictions'].plot()

#plot

plt.figure(figsize=(15, 8))

valid['Predictions'] = 0
valid['Predictions'] = preds
plt.title("Moving Average Prediction of Last Years by Training by its prevoous 4 years")
plt.ylabel("INFY Stock Price")
plt.xlabel("Date")
plt.plot(train['open'])
plt.plot(valid[['open', 'Predictions']])
plt.legend(["Train Data","Test Data","Prediction"])
plt.show()

"""Insight / Inference :     
- Predicted line  is very much similar that of the actual
line exist of last year 
- Thus Moving Average has been Applied to predict stock trend

## Analysis and Using Different Time Series Methods

### Simple Moving Average (SMA) 
- It is majorally used in stock market short-term trading in-which when our SMA( i.e smooth curve ) cuts the original line/graph then the trader can make decision of buying/sellings the stocks
"""

# Simple moving average
# ## .rolling(window-size,minimum-period)
# - window-size    :-  5  - Icluding that value and above  it will take mean of 5 then replace there
# - minimum-period :-  5  - starting  4 values become nan

# Modifications to the data or indices of the copy will not be reflected in the original object

df_infy = df.copy() 

df_infy['open:10 days rooling '] = df_infy['open'].rolling(window=10,min_periods=2).mean()
df_infy['open:30 days rooling '] = df_infy['open'].rolling(window=30,min_periods=2).mean()
df_infy[['open','open:10 days rooling ','open:30 days rooling ']].head(2)

df_infy.columns

# Lets plot for above cols
# Jan Month of 2021
df_infy[['open','open:10 days rooling ','open:30 days rooling ']].plot(xlim=['2022-01-01','2022-04-19'])
# Here its not clearly visible  so , look into next cell output

"""### EMA(exponencial moving average)
- formula :
((close_value - prev_ema_value )*multiplier ) + prev_ema_value
- formula for multiplier:
(2/( Window_rolling_size_value )+1)

- Since its req prev ema value so
- for 1st value we need to do sma with cosidering rolling_window_size same as to used while calculating multiplier

"""

df_infy['EMA_0.1'] = df_infy['open'].ewm(alpha=0.1,adjust=False).mean()
# lets do for diff  smothening factor
df_infy['EMA_0.2'] = df_infy['open'].ewm(alpha=0.2,adjust=False).mean()
df_infy['EMA_0.3'] = df_infy['open'].ewm(alpha=0.3,adjust=False).mean()
df_infy[['EMA_0.1','EMA_0.2','EMA_0.3']].plot()

df_infy

# LETS plot from  jan month of 2022 till now
df_infy[['open','EMA_0.1','EMA_0.2','EMA_0.3']]['2022-04-01 00:00:00+00:00':'2022-04-20 00:00:00+00:00'].plot()

"""### EWMA :
- Formula :
#### EMWA(t) = a* x(t) + (1-a)*EMWA(t-1)
<br>   
- here a => weight  (providing more imp for current data) & to prevent lag
"""

# Lets plot EWMA
df_infy['EWMA'] = df_infy['open'].ewm(span=5).mean()
df_infy[['open','EWMA']]['2022-04-01 00:00:00+00:00':].plot()

"""- Insight :     
     -  Always SMA , EMA and EWMA

# Linear Regression
- linear regression is a linear approach for modelling the relationship between a scalar response and one or more explanatory variables (also known as dependent and independent variables).

## Predicting the curve for the previous year from last 4 years
"""

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split


#setting index as date values
df['date'] = pd.to_datetime(df.date,format='%Y-%m-%d')
df.index = df['date']

#sorting
data = df.sort_index(ascending=True, axis=0)

#creating a separate dataset
new_data = pd.DataFrame(index=range(0,len(df)),columns=['date', 'open'])

for i in range(0,len(data)):
    new_data['date'][i] = data['date'][i]
    new_data['open'][i] = data['open'][i]

import fastai
print(fastai.__version__)

"""This creates features such as:

‘Year’, ‘Month’, ‘Week’, ‘Day’, ‘Dayofweek’, ‘Dayofyear’, ‘Is_month_end’, ‘Is_month_start’, ‘Is_quarter_end’, ‘Is_quarter_start’,  ‘Is_year_end’, and  ‘Is_year_start’.
"""

#create features

from fastai.tabular import * 
new_data = add_datepart(new_data,'date')
new_data.drop('Elapsed', axis=1, inplace=True)  #elapsed will be the time stamp

new_data.columns

# split into train and validation
train = new_data[:1007]
valid = new_data[1007:]

x_train = train.drop('open', axis=1)
y_train = train['open']
x_valid = valid.drop('open', axis=1)
y_valid = valid['open']

# implement linear regression
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(x_train,y_train)

#make predictions and find the rmse
preds = model.predict(x_valid)
rms=np.sqrt(np.mean(np.power((np.array(y_valid)-np.array(preds)),2)))
rms

"""- As previous Moving Average rms was 6.65 and LR having more than that 
- The RMSE value is higher than the previous technique, which clearly shows that linear regression has performed poorly. Let’s look at the plot and understand why linear regression has not done well:
"""

#plot
plt.figure(figsize=(15, 8))

valid['Predictions'] = 0
valid['Predictions'] = preds
plt.title("Linear Regression  Prediction of Last Years by Training by its prevoous 4 years")
valid.index = new_data[1007:].index
train.index = new_data[:1007].index
plt.ylabel("INFY Stock Price")
plt.xlabel("Date")
plt.plot(train['open'])
plt.plot(valid[['open', 'Predictions']])
plt.legend(["Train Data","Test Data","Prediction"])

plt.show()

"""- Inference :    

Linear regression is a simple technique and quite easy to interpret, but there are a few obvious disadvantages. One problem in using regression algorithms is that the model overfits to the date and month column. Instead of taking into account the previous values from the point of prediction, the model will consider the value from the same date a month ago, or the same date/month a year ago.

## Now we will Use LR for prediction the value of future day's price based on historic details
"""

df_infy = df.copy()
df_infy.sample(2)

df_infy.columns

df_infy.drop(['symbol'],inplace=True,axis=1)
df_infy.drop(['date'],inplace=True,axis=1)
df_infy.columns

#Let's select our features
x = df_infy.copy()
x.drop(['open'],inplace=True,axis=1)
y = df.loc[:,'open']


print(x.columns)
y.sample(2)

from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.1,random_state = 0)

# now fitting the LR model 
LR = LinearRegression()
LR.fit(x_train,y_train)
tree = DecisionTreeRegressor().fit(x_train, y_train)

y_pred_lr = LR.predict(x_test)
y_pred_dt = tree.predict(x_test)
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
print(' FOR Linear Regression ')
print("MAE :",mean_absolute_error(y_test,y_pred_lr))
print("MSE :",mean_squared_error(y_test,y_pred_lr))
print("RMSE :",np.sqrt(mean_squared_error(y_test,y_pred_lr)))
r2 = r2_score(y_test,y_pred_lr)
print("R2 Squared :",r2)

print();print()
print(' FOR Decision Tree ')
print("MAE :",mean_absolute_error(y_test,y_pred_dt))
print("MSE :",mean_squared_error(y_test,y_pred_dt))
print("RMSE :",np.sqrt(mean_squared_error(y_test,y_pred_dt)))
r2 = r2_score(y_test,y_pred_dt)
print("R2 Squared :",r2)

result_dataframe_LR = pd.DataFrame({'Actual':y_test,'Predicted':y_pred_lr})
result_dataframe_LR.plot(figsize=(12,5))

result_dataframe_LR.tail(20).plot(kind='bar',figsize=(12,5))

result_dataframe_DT = pd.DataFrame({'Actual':y_test,'Predicted':y_pred_dt})
result_dataframe_DT.plot(figsize=(12,5))

result_dataframe_DT.tail(20).plot(kind='bar',figsize=(12,5))

# Predicting open value of stock in 'x' days in future 
future_days  = 30 

df_infy_open = pd.DataFrame(df['open'])
df_infy_open['Prediction'] = df_infy_open['open'].shift(-future_days)
df_infy_open

# Creating a feature data set (X) and convert it to a numpy array  and remove the last 'x' rows/day's
X = np.array(df_infy_open.drop(['Prediction'], 1))[:-future_days]
print(X)

# Creating a target data set (y) and convert it to a numpy array  and to get the target values except the last 'x' rows/day's
y = np.array(df_infy_open['Prediction'])[:-future_days]
print(y)

"""#### Predicting future 30 days with an Linear and Decision Tree"""

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)

from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression

# Implementing Linear and Decision Tree Regression Algorithms.
tree = DecisionTreeRegressor().fit(x_train, y_train)
lr = LinearRegression().fit(x_train, y_train)

# Get the last 'x' rows of the feature data set 
x_future = df_infy_open.drop(['Prediction'], 1)[:-future_days]
x_future = x_future.tail(future_days)
x_future = np.array(x_future)
x_future

tree_prediction = tree.predict(x_future)
print(tree_prediction)

predictions = tree_prediction 
valid = df_infy_open[X.shape[0]:]
valid['Predictions_dt'] = predictions

lr_prediction = lr.predict(x_future)
print(lr_prediction)

predictions_lr = lr_prediction 
valid = df_infy_open[X.shape[0]:]
valid['Predictions_lr'] = predictions_lr

df_infy_open.open

"""##### Thsi is the DT prediction graph for next 30 days 

"""

plt.figure(figsize=(16,8))
plt.title("Using Decision Tree To Predict next 30 days Stock value")
plt.xlabel('Days')
plt.ylabel('Open Price ')
plt.plot(df_infy_open['open'])
valid['Predictions_dt'] = predictions
plt.plot(valid[['open', 'Predictions_dt']])
plt.legend(["Original", "Valid", 'Predicted Using DT'])
plt.show()

"""##### Thsi is the LR prediction graph for next 30 days 

"""

plt.figure(figsize=(16,8))
plt.title("Using Linear Regression To Predict next 30 days Stock value")
plt.xlabel('Days')
plt.ylabel('Open Price ')
plt.plot(df_infy_open['open'])
plt.plot(valid[['open', 'Predictions_lr']])
plt.legend(["Original", "Valid", 'Predicted Using LR'])
plt.show()

"""# k-Nearest Neighbours

- k-nearest neighbor algorithm:
- This algorithm is used to solve the classification model problems. K-nearest neighbor or K-NN algorithm basically creates an imaginary boundary to classify the data. When new data points come in, the algorithm will try to predict that to the nearest of the boundary line.

- Therefore, larger k value means smother curves of separation resulting in less complex models. Whereas, smaller k value tends to overfit the data and resulting in complex models.

- Note: It’s very important to have the right k-value when analyzing the dataset to avoid overfitting and underfitting of the dataset.

-Using the k-nearest neighbor algorithm we fit the historical data (or train the model) and predict the future.

## Taking Prev  4 yrs a trainin  and then predict prev 1 yr
"""

from sklearn.model_selection import train_test_split
# Machine learning libraries
from sklearn.metrics import accuracy_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
 

 # split into train and validation
train = new_data[:1007]
valid = new_data[1007:]

x_train = train.drop('open', axis=1)
y_train = train['open']
x_valid = valid.drop('open', axis=1)
y_valid = valid['open']


 #scaling data
x_train_scaled = scaler.fit_transform(x_train)
x_train = pd.DataFrame(x_train_scaled)
x_valid_scaled = scaler.fit_transform(x_valid)
x_valid = pd.DataFrame(x_valid_scaled)

#using gridsearch to find the best parameter
params = {'n_neighbors':[2,3,4,5,6,7,8,9]}
knn = neighbors.KNeighborsRegressor()
model = GridSearchCV(knn, params, cv=5)

#fit the model and make predictions
model.fit(x_train,y_train)
preds = model.predict(x_valid)
 
 #rmse
rms=np.sqrt(np.mean(np.power((np.array(y_valid)-np.array(preds)),2)))
rms

#plot
valid['Predictions'] = 0
valid['Predictions'] = preds
plt.plot(valid[['open', 'Predictions']])
plt.plot(train['open'])

"""## Predicting future 30 days with an KNN"""

# Predicting open value of stock in 'x' days in future 
future_days  = 30 

df_infy_open = pd.DataFrame(df['open'])
df_infy_open['Prediction'] = df_infy_open['open'].shift(-future_days)
# Creating a feature data set (X) and convert it to a numpy array  and remove the last 'x' rows/day's
X = np.array(df_infy_open.drop(['Prediction'], 1))[:-future_days]
# Creating a target data set (y) and convert it to a numpy array  and to get the target values except the last 'x' rows/day's
y = np.array(df_infy_open['Prediction'])[:-future_days]
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)

#scaling data
x_train_scaled = scaler.fit_transform(x_train)
x_train = pd.DataFrame(x_train_scaled)
# x_valid_scaled = scaler.fit_transform(x_test)
# x_valid = pd.DataFrame(x_valid_scaled)

#using gridsearch to find the best parameter
params = {'n_neighbors':[2,3,4,5,6,7,8,9]}
knn = neighbors.KNeighborsRegressor()
model = GridSearchCV(knn, params, cv=5)

#fit the model and make predictions
model.fit(x_train,y_train)

# Get the last 'x' rows of the feature data set 
x_future = df_infy_open.drop(['Prediction'], 1)[:-future_days]
x_future = x_future.tail(future_days)
x_future = np.array(x_future)
x_future

predictions_knn = model.predict(x_future)

valid = df_infy_open[X.shape[0]:]
valid['Predictions_knn'] = predictions_knn

plt.figure(figsize=(16,8))
plt.title("Using KNN To Predict next 30 days Stock value")
plt.xlabel('Days')
plt.ylabel('Open Price ')
plt.plot(df_infy_open['open'])
valid['Predictions_knn'] = predictions
plt.plot(valid[['open', 'Predictions_knn']])
plt.legend(["Original", "Valid", 'Predicted Using KNN'])
plt.show()